---
title: "Untitled"
output:
  html_document: default
  word_document: default
  pdf_document: default
---
```{r}
#En esta celda cargaremos todas las librerias necesarias
if (!require(tictoc)) install.packages('tictoc')
library(tictoc)

if (!require(sabre)) install.packages('sabre')
library(sabre)

if (!require(tidyverse)) install.packages('tidyverse')
if (!require(tidyr)) install.packages('tidyr')
library(tidyr)
library(ggplot2)
```

El primer paso de todos será cargar nuestro dataset
```{r}
tic('reading time:')
brain <- read.csv('Brain_GSE50161.csv')
toc()
```

Como podemos observar, el tiempo de carga es bastante largo, debido al gran tamaño de nuestro dataset.  
Una vez hayamos cargado el dataset, lo primero que queremos saber son algunos de los parametros que lo definen, como el tamaño, el estado (si existen missing values), el tipo de valores que tenemos, los valores maximos y minimos, etc.
```{r}
rows <- nrow(brain)
cols <- ncol(brain)
missing <- sum(is.na(brain))
maximos <- c()
minimos <- c()
iqrango <- c()
for (i in 3:cols){
  maximos <- c(maximos, max(brain[,i]))
  minimos <- c(minimos, min(brain[,i]))
  iqrango <- c(iqrango, IQR(brain[,i]))
}

rows
cols
missing

hist(iqrango, main='Histograma para el rango de las variables')
```
Vemos que no existen missing values y existen 54677 columnas. en cuanto al rango, vemos que ninguno de los genes tiene un rango muy superior al resto, estando todos en
Una vez conocidos estos parametros, debemos decidir si será necesario normalizar los datos, o si por el contrario, las varianzas no son demasiado diferentes.
```{r}
varianzas <- sapply(brain[,3:cols], var)
hist(varianzas, main = 'Histograma de las varianzas')
mean(varianzas)
max(varianzas)
```
Aunque pueda parecer necesario normalizar los datos, en este caso no vamos a hacerlo. El motivo es que el intento de normalizar los datos de un microarray supone una importante perdida de informacion a la hora de trabajar con los datos

Una vez conocidos estos parametros, podemos observar que el dataset tiene un total de 130 observaciones y 54677 columnas o variables. Existe un claro problema de dimensionalidad, que queremos solventar cuanto antes.



#Preparacion de los datos

El metodo que vamos a llevar a cabo para reducir la dimensionalidad del problema es el de Analisis de Componentes Principales. Este metodo es util para lidiar con problemas de gran dimensionalidad. Consiste en obtener, a partir de un set de variables colineales entres si, un nuevo set de variables o _Componentes Principales_ que son ortogonales entre si. Estas nuevas variables están ordenadas de mayor a menor varianza, siendo la primera componente la que tiene la mayor proporcion de varianza de la muestra. Utilizar todas las componentes principales para un analisis proporciona la misma cantidad de informacion que utilizar el dataset original.
Gracias a este metodo, podemos reducir de manera notable la dimensionalidad del problema sin perder información.

```{r}
pca<- prcomp(brain[3:cols]) #Realizamos con esta linea un analisis de componentes principales
proportion <- summary(pca)$importance[2,] #Almacenamos en un vector la
#proporcion de la varianza total que tiene cada componente principal
sum_prop <- cumsum(proportion) # Queremos obtener un vector con la suma cumulativa de la importancia de cada componente principal para luego poder representarlas graficamente

plot(x = 1:length(sum_prop),         
     y = sum_prop,
     main = "Proporcion de la varianza acumulada",
     xlab = "Número de CPs",
     ylab = "Varianza acumulada")
```

Este plot nos indica cuanta proporcion de la varianza total obtenemos utilizando las X primeras componentes principales. Como podemos ver, con 20 componentes principales obtenemos un 70% de la varianza, lo que en casos como este, de gran dimensionalidad, es aceptable.

# Evaluacion del modelo
A continuacion vamos a realizar una evaluacion de distintos modelos de clustering para los datos. Dicha evaluacion la vamos a realizar sobre los datos obtenidos del analisis de componentes principales.

La evaluacion se realizara en funcion a la medida "V-measure". Se trata de una medida externaliazada para problemas de clustering con etiquetado. Se basa en una combinacion de las medidas de homogeneidad y completitud. La homogeneidad se define como asignar a un cluster __solo__ datos de un grupo, mientras que la completitud se defino como asignar __todos__ los datos de un grupo al mismo cluster. La medida "V-measure" se define como la media de estas dos medidas [Rosenberg & Hirschberg, 2007].

##Hierarchical clustering
Realizamos la evaluacion del clustering jerárquico teniendo en cuenta todas las posibles combinaciones de factores:
  -Utilizamos las 7 distancias posibles
  -Utilizamos las 3 maneras posibles de dividir el arbol en 5
Tambien realizaremos un barrido por todas las componentes principales, utilizando las X primeras componentes principales para cada caso.
```{r}
pca_labels <- brain[,2] #Obtenemos las etiquetas reales de cada individuo para ser luego capaces de comparar con las predichas y obtener una puntuacion V-measure
groups <- unique(pca_labels)#Almacenamos de manera individual cada uno de los tipos de cancer que existen. Existen 5 tipos.
labels_numeric <- unclass(as.factor(pca_labels))#Transformamos las etiquetas reales de cada individuo en un factor de 1 a 5
k_true <- length(unique(pca_labels))#Obtenemos el numero real de factores o grupos que existe en el dataset.


#Definimos las posibilidades para calcular de manera distinta el cluster jerárquiqo:
  #Distintas maneras de calcular la matriz de distancias.
distances <- c("euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski")
  #Distintas maneras de partir el arbol jerárquico.
clust_methods <- c('single','complete','average')

#Definimos un dataframe con las columnas que queremos.
columns <- c('distance','clust_method','PCs','Homogeneity','Completeness','V.score')
hierarchical_scores <- data.frame(matrix(ncol = 6, nrow = 0))


#En este loop realizamos por cada distancia, por cada particion, y por cada X primeras componentes principales un clustering jerárquico de los datos obtenidos del analisi de componentes principales.
for (j in distances){
  for (i in 1:130){
    pca_datos <- data.frame(pca$x[,1:i])
    dist_matrix <- dist(pca_datos, method = j)
    for (q in clust_methods){
      tree <- hclust(dist_matrix,method = q)
      labels_h <- cutree(tree, k = k_true)
      scoring_h <- vmeasure(labels_numeric,labels_h)
      
      #Aqui, añadimos al dataframe una linea por cada iteracion, con la informacion del metodo de clustering y las puntuaciones de completeness, homogeneity y V_measure.
      newrow <- c(j,q,i,
                  scoring_h$homogeneity,
                  scoring_h$completeness,
                  scoring_h$v_measure)
      hierarchical_scores <- rbind(hierarchical_scores,newrow)
      
    }
  }
}

#Ordenamos el dataframe con 6 columnas, en un tibble que tendra 3 grupos, uno por cada metodo de particionado.
colnames(hierarchical_scores) <- columns
hierarchical_scores[,3:6] <- lapply(hierarchical_scores[,3:6], as.numeric)
tidied <-  hierarchical_scores %>% group_by(distance) %>% group_by(clust_method)%>% nest()
```



Ahora, representaremos los resultados obtenidos
#Plotting


```{r}
for (i in 1:3){
  data <- (tidied$data[[i]])
  plot <- data %>%
    ggplot( aes(x = PCs, y = V.score, group = distance, color =
                  distance))+
    geom_line() + ggtitle(clust_methods[i])
  print(plot)
}
```
Como podemos ver, el metodo de particionado "single" reporta unos resultados muy variables, y tan solo cuando utilizamos las 20 primeras componentes principales. El metodo "complete" devuelve unos resultados algo mas estables, rondando el 0.5
para todas las distancias. Por último, el metodo de particionado "average" reporta unos resultados muy buenos para la distancia "manhattan" antes de llegar a las 25 componentes principales, que luego desciende a casi 0. 

Como podemos observar en estos graficos, para este problema en particular, el metodo de aglomeracion jerárquico no parece ideal, puesto que presenta una gran variabilidad dependiendo del numero de componentes principales que utilicemos. Esto puede significar que no estamos utilizando de manera correcta todos los datos de los que disponemos.


# K-means
A continuación realizaremos un estudio del metodo de aglomeracion por k-vecinos.

```{r}
#Definimos un dataframe con las columnas deseadas.
k_scores <- data.frame(matrix(ncol = 4, nrow = 0))
columns_k <- c('PCs','Homogeneity','Completeness','V.score')


for (j in 1:130){
  pca_datos <- data.frame(pca$x[,1:j])#Obtenemos las j primeras PCs
  
  k_clust <- kmeans(pca_datos, centers = 5,nstart = 50)#Realizamos un clustering con las j primeras componentes principales, 5 centros, y nstart = 50. nstart indica el numero de veces que queremos que se inicien semillas aleatorias y devuelve el mejor resultado.
  
  labels_k <- k_clust$cluster
  #Obtenemos la V-measure
  scoring <- vmeasure(labels_numeric,labels_k) #Calculamos la score
  
  #Añadimos una linea por cada iteracion a nuestro dataframe
  newrow_k <- c(j, scoring$homogeneity,
              scoring$completeness,
              scoring$v_measure)
  names(newrow_k) <- columns_k
  k_scores <- rbind(k_scores,newrow_k)
  
}  
colnames(k_scores) <- columns_k
```

#Plotting
```{r}
k_scores %>% ggplot(aes(x = PCs, y = V.score))+ geom_line()+ggtitle('K-means')
```


Como podemos observar, los resultados a partir de 6 componentes principales comienzan a estabilizarse, y alcanzan un comportamiento asintótico a partir de entonces. El resultado nunca sube por encima de 0,65.

Una vez evaluados los dos metodos de aglomeración, debemos decidir cual es mejor.
```{r}
hierarchical_scores[which.max(hierarchical_scores$V.score),]
k_scores[which.max(k_scores$V.score),]

```
Para el clustering jeárquico, obtenemos que la mejor puntuacion se obtiene empleando 14 componentes principales, midiendo la distancia de "Manhattan" y dividiendo el arbol segun el metodo "average"

Para el clustering k-vecinos, obtenemos que el mejor resultado se obtiene a partir de 11 componentes principales.

# Correcciones
## 2 suposiciones que hemos tomado como ciertas
Una vez hecho un analisis exploratorio de los datos y de los principales metodos de clustering a utilizar, vamos a intentar optimizar tanto los datos como la metodologia para obtener unos resultados mas favorables. Nos serviremos del hecho de que estamos lidiando con un problema de clustering, pero los datos están etiquetados.

### Debemos usar todos los genes que tenemos a nuestra disposicion para realizar el clustering
A continuacion, sirviendonos de la informacion que nos brinda el dataset, elegiremos los genes que son mas distintivos de cada tipo de cancer, que son aquellos con el menor within class variance, pero con el mayor between class variance. El test que se realiza para este problema se denomina "analisis de la varianza" o ANOVA por sus siglas en ingles. 
Queremos comprobar si existe alguno de los genes que no varie entre distintos tipos de cancer, es decir, que la media de la expresion de todos los genes sea la misma para el tipo de cancer que sea. Definimos el analisis de la siguiente manera:
$H_{0}: \mu_{ependymoma} = \mu_{glioblastoma} = \mu_{medulloblastoma} = \mu_{normal} = \mu_{pilocytic_astrocytoma }$
$H_{1}:$ no todas las medias son iguales.

El test nos devolvera un valor p (p-value) para el estadistico F que se calcula para cada una de las variables. Si el valor p es menor que 0.05 (valor de significancia elegido), podemos afirmar que para dicho gen, existe una diferencia estadística en su expresión para al menos uno de los grupos. 

```{r}
mask <- c(0,1) #Primero, creamos el vector que va a ser nuestra mascara para el dataframe. Incluimos la segunda columna (tipo de cancer)

#En este loop, vamos a hacer un barrido de todas las columnas del dataset brain
for (i in colnames(brain)[3:cols]){
  #Realizamos un nuevo dataset que ira cambiando cada iteracion. En este dataset incluimos las etiquetas de cada individuo y la informacion de uno de los genes.
  dummy <- data.frame(brain[2], brain[i])
  colnames(dummy) <- c('type','gene')
  
  #Ahora, realizamos el test ANOVA de dicho gen con respecto a las etiquetas y obtenemos el p-valor
  anova_gene <- aov(gene~type, data=dummy)
  pvalue <- summary(anova_gene)[[1]][1,5]
  
  #Si dicho p-valor es menor que 0.05, significa que la diferencia de las medias de cada grupo para dicho gen es significativa, por lo que incluimos el gen en nuestro analisis.
  if (pvalue < 0.05){
    mask <- c(mask, 1)
  }
  else{
    mask <- c(mask,0)
  }
}
sum(mask)
```

El resultado final nos dice que solo 37367 genes de los 54677 genes son representativos desde un punto de vista estadistico. Siguen siendo muchos, pero hemos eliminado casi la mitad de ellos. 

### Nuevo PCA
```{r}
brain_redux <- brain[,as.logical(mask)]
pca_redux<- prcomp(brain_redux[2:ncol(brain_redux)])
brain_red_pca <- pca_redux$x
```

#### Hierarchical clustering (best result)
```{r}
pca_labels <- brain[,2] #Obtenemos las etiquetas reales de cada individuo para ser luego capaces de comparar con las predichas y obtener una puntuacion V-measure
groups <- unique(pca_labels)#Almacenamos de manera individual cada uno de los tipos de cancer que existen. Existen 5 tipos.
labels_numeric <- unclass(as.factor(pca_labels))#Transformamos las etiquetas reales de cada individuo en un factor de 1 a 5
k_true <- length(unique(pca_labels))#Obtenemos el numero real de factores o grupos que existe en el dataset.


#Definimos las posibilidades para calcular de manera distinta el cluster jerárquiqo:
  #Distintas maneras de calcular la matriz de distancias.
distances <- c("euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski")
  #Distintas maneras de partir el arbol jerárquico.
clust_methods <- c('single','complete','average')

#Definimos un dataframe con las columnas que queremos.
columns <- c('distance','clust_method','PCs','Homogeneity','Completeness','V.score')
hierarchical_scores <- data.frame(matrix(ncol = 6, nrow = 0))


#En este loop realizamos por cada distancia, por cada particion, y por cada X primeras componentes principales un clustering jerárquico de los datos obtenidos del analisi de componentes principales.
for (j in distances){
  for (i in 1:130){
    pca_datos <- data.frame(brain_red_pca[,1:i])
    dist_matrix <- dist(pca_datos, method = j)
    for (q in clust_methods){
      tree <- hclust(dist_matrix,method = q)
      labels_h <- cutree(tree, k = k_true)
      scoring_h <- vmeasure(labels_numeric,labels_h)
      
      #Aqui, añadimos al dataframe una linea por cada iteracion, con la informacion del metodo de clustering y las puntuaciones de completeness, homogeneity y V_measure.
      newrow <- c(j,q,i,
                  scoring_h$homogeneity,
                  scoring_h$completeness,
                  scoring_h$v_measure)
      hierarchical_scores <- rbind(hierarchical_scores,newrow)
      
    }
  }
}

#Ordenamos el dataframe con 6 columnas, en un tibble que tendra 3 grupos, uno por cada metodo de particionado.
colnames(hierarchical_scores) <- columns
hierarchical_scores[,3:6] <- lapply(hierarchical_scores[,3:6], as.numeric)
tidied <-  hierarchical_scores %>% group_by(distance) %>% group_by(clust_method)%>% nest()
```

Realizamos la evaluacion del clustering jerárquico teniendo en cuenta todas las posibles combinaciones de factores:
  -Utilizamos las 7 distancias posibles
  -Utilizamos las 3 maneras posibles de dividir el arbol en 5
Tambien realizaremos un barrido por todas las componentes principales, utilizando las X primeras componentes principales para cada caso.

Ahora, representaremos los resultados obtenidos
#Plotting
```{r}
A <- function(x){
  x_trunc <- signif(x,2)
  return(x_trunc)
}
```

```{r}
for (i in 1:3){
  data <- (tidied$data[[i]])
  plot <- data %>%
    ggplot( aes(x = PCs, y = V.score, group = distance, color =
                  distance))+
    geom_line() + ggtitle(clust_methods[i])
  print(plot)
}
```


#### k-means Clustering
```{r}
#Definimos un dataframe con las columnas deseadas.
k_scores <- data.frame(matrix(ncol = 4, nrow = 0))
columns_k <- c('PCs','Homogeneity','Completeness','V.score')


for (j in 1:130){
  pca_datos <- data.frame(brain_red_pca[,1:j])#Obtenemos las j primeras PCs
  
  k_clust <- kmeans(pca_datos, centers = 5,nstart = 50)#Realizamos un clustering con las j primeras componentes principales, 5 centros, y nstart = 50. nstart indica el numero de veces que queremos que se inicien semillas aleatorias y devuelve el mejor resultado.
  
  labels_k <- k_clust$cluster
  #Obtenemos la V-measure
  scoring <- vmeasure(labels_numeric,labels_k) #Calculamos la score
  
  #Añadimos una linea por cada iteracion a nuestro dataframe
  newrow_k <- c(j, scoring$homogeneity,
              scoring$completeness,
              scoring$v_measure)
  names(newrow_k) <- columns_k
  k_scores <- rbind(k_scores,newrow_k)
  
}  
colnames(k_scores) <- columns_k
```

#Plotting
```{r}
k_scores %>% ggplot(aes(x = PCs, y = V.score))+ geom_line()+ggtitle('K-means')
```

Como podemos observar, el resultado del clustering jerárquico no mejor notablemente, pero sí lo hace el resultado del clustering k-means.

### Existen 5 grupos en los que realizar el clustering.
Tambien debemos cuestionar esta suposicion, puesto que es posible que dos canceres sean geneticamente muy similares, o bien que dentro del diagnostico de uno de los canceres, existan dos variantes geneticas o mas distintas. 
Por ello vamos a realizar un analisis del numero de clusters que minimizará el within-groups sum of squares contra el numero de clusters.

```{r}
n <- nrow(brain_redux)
wss <- rep(2:10)
for (i in 2:10){
  wss[i-1] <- sum(kmeans(brain_redux[2:ncol(brain_redux)], centers = i)$withinss)
}

plot(2:10, wss, type = 'b', xlab='Número de clusters', ylab='Within group sum of squares')
```

No obtenemos evidencia de que el numero correcto de clusters no sea 5, por lo que proseguiremos nuestro analisis asumiendo que los 5 tipos de cancer que se manifiestan en el dataset son los 5 clusters que queremos encontrar.



#Predictor de cancer: 
A continuación, desarrollamos un codigo para, sirviendonos del trabajo anterior, ser capaces de definir que tipo de cancer sufre una persona al que se le haya hecho un analisis genómico por microarray.

```{r}
#Definimos dos funciones que utilizaremos con posterioridad. 
  #La primera es la funcion getMode, que nos permitirá obtener la moda de un vector (valor que se repite mas veces)
getMode <- function(x){
  ux <- unique(x)
  ux[which.max(tabulate(match(x,ux)))]
}
  #La segunda es la funcion closest.cluster, que nos permitirá obtener que centrouide es mas cercano a un punto.
closest.cluster <- function(x){
    cluster.dist<- apply(km$centers, 1, function(y)
      dist(rbind(x,y)))
    return(which.min(cluster.dist)[1])
}
```

```{r}
# Definimos un dataset "percentages" que definira la proporcion de fallos y aciertos para cada tipo de cancer.
percentages <- data.frame(matrix(0L,ncol = 5, nrow = 2))
rownames(percentages) <- c('aciertos','fallos')
colnames(percentages) <- groups

#Realizamos una iteracion 100 veces:
for (i in 1:100){
  n <- rows #definimos el numero de filas del dataset
  n_train <- round(0.95* n)  #Definimos el porcentaje de datos a utilizar para el entrenamiento.
  
  #Definimos los datasets "train" y "test"
  train_indices <- sample(1:n, n_train)
  test_indices <- c(1:n)[-train_indices]
  brain_train <- brain_redux[train_indices, ]  
  brain_test <- brain_redux[-train_indices, ]
  
  #Realizamos el clustering
  km <- kmeans(brain_train[,2:ncol(brain_train)], centers = 5, nstart = 10)

  #Calculamos a que cluster pertenece cada uno de los datos del set test.
  clusters2 <- apply(brain_test[,2:ncol(brain_test)], 1, closest.cluster)
  clusters_lab <- clusters2
  
  #Realizamos un mapeado para ser capaces de etiquetar los datos de test
  mapping <- data.frame(c(1,2,3,4,5))
  mapping$class <- c(getMode(brain_train[km$cluster == 1,1]),
                    getMode(brain_train[km$cluster == 2,1]),
                    getMode(brain_train[km$cluster == 3,1]),
                    getMode(brain_train[km$cluster == 4,1]),
                    getMode(brain_train[km$cluster == 5,1]))
  
  for (i in 1:5){
  clusters2[clusters2 == i] <- mapping[mapping[,1] == i ,2]
  }
  
  #Por ultimo, rellenamos la matriz percentages, que nos dira las proporciones de aciertos y fallos para cada clase
  for (j in groups){
    good <- sum(clusters2[clusters2== j] == brain_test[clusters2 == j,1])
    bad <- length(clusters2[clusters2 == j])-good
    percentages[1,j] <- percentages[1,j] + good
    percentages[2,j] <- percentages[2,j] + bad
  }
}
```
```{r}
acierto_prop <- sum(percentages[1,])/(sum(percentages[1,])+sum(percentages[2,]))
acierto_prop
```

Como podemos observar, obtenemos un porcentaje de acierto del 88%, un muy buen resultado. Sin embargo, si analizamos la matriz percentages, nos damos cuenta de algo:
```{r}
percentages
```

```{r}
stacked <- data.frame(matrix(0L,ncol = 3, nrow = 10))
colnames(stacked) <- c('type','acierto','cant')
stacked$type <- rep(groups,1,each=2)
stacked$acierto <- rep(c(1,0),5)
stacked[stacked[,2]==1,3] <- as.numeric(percentages[1,])
stacked[stacked[,2]==0,3] <- as.numeric(percentages[2,])
```
```{r}
ggplot(data  = stacked, aes(fill=as.factor(acierto), y=cant, x=type)) + 
   geom_bar(position="fill", stat = 'identity')+
    scale_fill_manual(values=c("red", "green"),labels = c('fallos','aciertos'))+ggtitle('Porcentaje de acierto en la clasificación')+theme(text = element_text(size=10))+guides(fill=guide_legend(title=NULL))

```

Observamos que mientras que para los grupos "ependymoma", "medulloblastoma" y "normal", el porcentaje de acierto es extremadamente alto, mientras que los grupos "glioblastoma" y "pilocytic_astrocytoma" tienen un porcentaje menor, o en el ultimo caso, ni siquiera existen predicciones. Esto puede ser debido a que exsita una gran proximidad entre ambos grupos, de manera que el grupo "glioblastoma" absorbe los casos de "pilocytic_astrocytoma". Vamos a comprobar esta suposicion creando un nuevo grupo que sustituya a estos dos, que llamaremos "gli_pilo". Este grupo incluye los casos de "glioblastoma" y "pilocytic_astrocytoma"
```{r}
#Cambiamos las etiquetas de "glioblastoma" y "pilocytic_astrocytoma" por una que incluya los dos grupos. 
brain_redux[brain_redux[,1] == 'glioblastoma'|
              brain_redux[,1] == 'pilocytic_astrocytoma',1] <- 'gli_pilo'
groups_4 <- unique(brain_redux[,1])

#A continuacion, replicamos el codigo anterior pero con tan solo 4 grupos.
percentages_4 <- data.frame(matrix(0L,ncol = 4, nrow = 2))
rownames(percentages_4) <- c('aciertos','fallos')
colnames(percentages_4) <- groups_4
  
for (i in 1:100){
  n <- rows
  n_train <- round(0.95* n) 
  train_indices <- sample(1:n, n_train)
  test_indices <- c(1:n)[-train_indices]
  brain_train <- brain_redux[train_indices, ]  
  brain_test <- brain_redux[-train_indices, ]
  
  km <- kmeans(brain_train[,2:ncol(brain_train)], centers = 4, nstart = 10)


  clusters2 <- apply(brain_test[,2:ncol(brain_test)], 1, closest.cluster)
  clusters_lab <- clusters2
  mapping <- data.frame(c(1,2,3,4))
  mapping$class <- c(getMode(brain_train[km$cluster == 1,1]),
                    getMode(brain_train[km$cluster == 2,1]),
                    getMode(brain_train[km$cluster == 3,1]),
                    getMode(brain_train[km$cluster == 4,1]))
  
  for (i in 1:4){
  clusters2[clusters2 == i] <- mapping[mapping[,1] == i ,2]
  }
  
  for (j in groups){
    good <- sum(clusters2[clusters2== j] == brain_test[clusters2 == j,1])
    bad <- length(clusters2[clusters2 == j])-good
    percentages_4[1,j] <- percentages_4[1,j] + good
    percentages_4[2,j] <- percentages_4[2,j] + bad
  }
}
```
```{r}
acierto_prop <- sum(percentages_4[1,])/(sum(percentages_4[1,])+sum(percentages_4[2,]))
acierto_prop
```
```{r}
percentages_4
```

```{r}
stacked_4 <- data.frame(matrix(0L,ncol = 3, nrow = 8))
colnames(stacked_4) <- c('type','acierto','cant')
stacked_4$type <- rep(groups_4,1,each=2)
stacked_4$acierto <- rep(c(1,0),4)
stacked_4[stacked_4[,2]==1,3] <- as.numeric(percentages_4[1,])
stacked_4[stacked_4[,2]==0,3] <- as.numeric(percentages_4[2,])
```
```{r}
ggplot(data  = stacked_4, aes(fill=as.factor(acierto), y=cant, x=type)) + 
   geom_bar(position="fill", stat = 'identity')+
   scale_fill_manual(values=c("red", "green"),labels = c('fallos','aciertos'))+ggtitle('Porcentaje de acierto en la clasificación')+theme(text = element_text(size=10))+guides(fill=guide_legend(title=NULL))
```